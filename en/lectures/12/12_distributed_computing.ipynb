{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed computing - Spark\n",
    "\n",
    "Spark is currently the first framework people are considering when they want to do an in-memory distributed data processing.\n",
    "\n",
    "However, there are many other possibilities, tools and technologies:\n",
    "* **Hadoop (MapReduce + HDFS)** - tool to consider when you have very large amounts of data, that would not fit to memory on even a large cluster or/and you want to process them on a disk\n",
    "* Storm\n",
    "* Flink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mattturck.com/wp-content/uploads/2019/07/2019_Matt_Turck_Big_Data_Landscape_Final_Fullsize.png\" alt=\"BigData landscape 2019\"/>\n",
    "source: https://mattturck.com/data2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - a few properties\n",
    "I will be using new keywords which you've probably already met in different courser, If we stumble upon one you don't understand, stop me.\n",
    "\n",
    "* In-memory data processing\n",
    "* Uniform access to the data and computational resources (I write the same code regardless whether for one computer or for a whole cluster)\n",
    "* Scala and JVM on the background, but has very good API accessible from Python and R\n",
    "* basically MapReduce but in memory and with the possibility to process in micro batches and as stream of data\n",
    "* The basis is RDD (Resilient distributed dataset) - a collection of data distributed to various computational nodes. The foundation of work are transformations on data represented as RDD. Simple support for basic operations such as map, filter, collect\n",
    "* Transformations are lazy. They are not executed until they are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://147.175.149.68:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData # this is an example of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.map(lambda x: x % 2 == 0)\n",
    "f.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.filter(lambda x: x % 2 == 0)\n",
    "f.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Toy example with searching primes\n",
    "\n",
    "I have a function testing the number whether it is a prime and I want to distribute it on a lot of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python\n",
    "\n",
    "def isprime(n):\n",
    "    n = abs(int(n))\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 4 ms, total: 28 ms\n",
      "Wall time: 3.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nums.filter(isprime).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Late evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 63.7 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nums.filter(isprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't call a function that returns results so nothing was executed. Only a RDD with transformations was prepared. These transformations will be executed when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 79.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nums.filter(isprime).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I have called a function, that requires to compute some results. It was enough to compute a few results, so only part of the computation was performed. Other functions starting the execution are for example collect, count, ...\n",
    "\n",
    "Be careful with these functions (mainly with collect). They return all the data that are the result of the computation. Even if it is a lot of data, they will still try to return it to the computer you use to access the SPark cluster from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 8 ms, total: 24 ms\n",
      "Wall time: 3.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[999983, 999979, 999961, 999959, 999953]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nums.filter(isprime).takeOrdered(5, key = lambda x: -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I needed to compute it all to be able to sort the results. The whole computation had to be executed and it took some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's try other, more real example\n",
    "\n",
    "sources:\n",
    "* https://github.com/jadianes/spark-py-notebooks\n",
    "* https://www.codementor.io/jadianes/python-spark-sql-dataframes-du107w74i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Download the data about attacks on a computer network\n",
    "\n",
    "The data describe connections in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"data/kddcup.data.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"data/kddcup.data_10_percent.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"data/corrected.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21M\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 1.4M Dec 11 23:39 corrected.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 2.1M Dec 11 23:32 kddcup.data_10_percent.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly  18M Dec 11 23:32 kddcup.data.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I will create the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that I am processing compressed data directly. The true amount is approx. 10 times higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We define schema - header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# I will load the data and set the schema\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I will create a DataFrame very similar to the dataframe in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "type(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I can use similar operations as with Pandas, but distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.groupBy(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What if I want to access it through SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So we can use the data for exploratory analysis on big amounts of data\n",
    "\n",
    "## We also have libraries to train some models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 4898431\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "print(\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 311029\n"
     ]
    }
   ],
   "source": [
    "test_data_file = \"data/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print(\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load it as list of rows = instances with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One instance looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '215',\n",
       "  '45076',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description is here\n",
    "http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## These are the values some attributes have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icmp', 'udp', 'tcp']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols = csv_data.map(lambda x: x[1]).distinct().collect()\n",
    "protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finger',\n",
       " 'netbios_dgm',\n",
       " 'name',\n",
       " 'X11',\n",
       " 'hostnames',\n",
       " 'vmnet',\n",
       " 'systat',\n",
       " 'shell',\n",
       " 'netstat',\n",
       " 'netbios_ssn',\n",
       " 'urh_i',\n",
       " 'pop_3',\n",
       " 'ldap',\n",
       " 'domain',\n",
       " 'mtp',\n",
       " 'remote_job',\n",
       " 'exec',\n",
       " 'supdup',\n",
       " 'courier',\n",
       " 'urp_i',\n",
       " 'pop_2',\n",
       " 'csnet_ns',\n",
       " 'smtp',\n",
       " 'whois',\n",
       " 'daytime',\n",
       " 'bgp',\n",
       " 'imap4',\n",
       " 'nntp',\n",
       " 'http_443',\n",
       " 'klogin',\n",
       " 'rje',\n",
       " 'IRC',\n",
       " 'link',\n",
       " 'http_8001',\n",
       " 'uucp',\n",
       " 'tftp_u',\n",
       " 'iso_tsap',\n",
       " 'uucp_path',\n",
       " 'auth',\n",
       " 'ecr_i',\n",
       " 'other',\n",
       " 'domain_u',\n",
       " 'ssh',\n",
       " 'discard',\n",
       " 'ctf',\n",
       " 'red_i',\n",
       " 'tim_i',\n",
       " 'time',\n",
       " 'login',\n",
       " 'Z39_50',\n",
       " 'ftp',\n",
       " 'telnet',\n",
       " 'ntp_u',\n",
       " 'sql_net',\n",
       " 'aol',\n",
       " 'private',\n",
       " 'gopher',\n",
       " 'efs',\n",
       " 'http_2784',\n",
       " 'ftp_data',\n",
       " 'nnsp',\n",
       " 'http',\n",
       " 'sunrpc',\n",
       " 'eco_i',\n",
       " 'harvest',\n",
       " 'kshell',\n",
       " 'echo',\n",
       " 'netbios_ns',\n",
       " 'pm_dump',\n",
       " 'printer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
    "services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0', 'RSTR', 'SH', 'S1', 'S2', 'RSTOS0', 'REJ', 'OTH', 'SF', 'S3', 'RSTO']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = csv_data.map(lambda x: x[3]).distinct().collect()\n",
    "flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's encode categorical data\n",
    "\n",
    "For simplicity, we can do it as they were ordinal. In the model, we will explicitly state, that these are categorical and not to be used as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41] (label)\n",
    "    clean_line_split = line_split[0:41]\n",
    "    \n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(protocols)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[2] = services.index(clean_line_split[2])\n",
    "    except:\n",
    "        clean_line_split[2] = len(services)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "        \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = csv_data.map(create_labeled_point)\n",
    "test_data = test_csv_data.map(create_labeled_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And we will train a decision tree\n",
    "\n",
    "We use library **mllib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 272.709 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "t0 = time.time()\n",
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n",
    "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
    "\n",
    "print(\"Classifier trained in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictions can be computed like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated in 0.049 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "\n",
    "print(\"Predictions generated in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And the accuracy can be evaluated like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 23.226 seconds. Test accuracy is 0.916\n"
     ]
    }
   ],
   "source": [
    "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "t0 = time.time()\n",
    "test_accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "\n",
    "print(\"Prediction made in {} seconds. Test accuracy is {}\".format(round(time.time() - t0,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rules from the tree can be extracted like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 27 nodes\n",
      "  If (feature 22 <= 33.0)\n",
      "   If (feature 25 <= 0.5)\n",
      "    If (feature 36 <= 0.48)\n",
      "     If (feature 34 <= 0.91)\n",
      "      Predict: 0.0\n",
      "     Else (feature 34 > 0.91)\n",
      "      Predict: 1.0\n",
      "    Else (feature 36 > 0.48)\n",
      "     If (feature 2 in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 1.0\n",
      "   Else (feature 25 > 0.5)\n",
      "    If (feature 3 in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 2 in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 3 not in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 38 <= 0.07)\n",
      "      Predict: 0.0\n",
      "     Else (feature 38 > 0.07)\n",
      "      Predict: 1.0\n",
      "  Else (feature 22 > 33.0)\n",
      "   If (feature 5 <= 0.0)\n",
      "    If (feature 2 in {19.0,35.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 not in {19.0,35.0})\n",
      "     If (feature 11 <= 0.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 11 > 0.0)\n",
      "      Predict: 0.0\n",
      "   Else (feature 5 > 0.0)\n",
      "    If (feature 29 <= 0.08)\n",
      "     If (feature 2 in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 29 > 0.08)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned classification tree model:\")\n",
    "print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, I can interpret the whole tree. I only have to look what is in individual attributes. The data description is here: http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
