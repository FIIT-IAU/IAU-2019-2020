{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distribuované počítanie - Spark\n",
    "\n",
    "Spark je v súčasnosti prvý framework, po ktorom ľudia idú ak chcú robiť nejaké distribuovane in-memory spracovanie údajov.\n",
    "\n",
    "Existuje ale veľmi veľa ďalších možností, nástrojov a technológii:\n",
    "* **Hadoop (MapReduce + HDFS)** - po tomto idú ľudia ak majú fakt veľa dát, ktoré nevlezú do pamäti ani na veľkom clustri a/alebo chcú pracovať na disku\n",
    "* Storm\n",
    "* Flink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://mattturck.com/wp-content/uploads/2019/07/2019_Matt_Turck_Big_Data_Landscape_Final_Fullsize.png\" alt=\"BigData landscape 2019\"/>\n",
    "zdroj: https://mattturck.com/data2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - zopár vlastností\n",
    "Budem používať neznáme termíny, ktoré ste ale možno počuli na iných predmetoch. Ak by ste narazili na nejaký termín, ktorému nerozumiete, tak ma zastavte.\n",
    "\n",
    "* In-memory spracovanie dát\n",
    "* Jednotný prístup k dátam a výpočtovým prostriedkom (rovnako píšem kód ak pracujem na mojom notebooku a na celom klastri)\n",
    "* Na pozadí Scala a JVM, ale má veľmi dobré API pre Python a aj R\n",
    "* v podstate MapReduce ale v pamäti a s možnosťou microbatchov na spracovanie prúdov dát\n",
    "* Základ je RDD (Resilient distributed dataset) - kolekcia dát distribuovaná na jednotlivé uzly. Základ práce sú transformácie nad dátami reprezentovanými ako RDD. Jednoduchá podpora pre základné operácie ako map, filter, collect\n",
    "* Transformácie sú lenivé (lazy). Nevykonávajú sa dokedy ich nepotrebujete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://147.175.149.68:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData # toto je prklad RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.map(lambda x: x % 2 == 0)\n",
    "f.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = distData.filter(lambda x: x % 2 == 0)\n",
    "f.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hračkársky príklad s hľadaním prvočísel\n",
    "\n",
    "Mám funkciu, ktorá overuje či je číslo prvočíslo a ja ju chcem distribuovať na veľa dát\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevzane z https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python\n",
    "\n",
    "def isprime(n):\n",
    "    n = abs(int(n))\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n",
      "Elapsed time: 2.577939033508301 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).count())\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Oneskorený výpočet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:48\n",
      "Elapsed time: 0.019066810607910156 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nezavolal som funkciu, ktorá by vracala výsledky, tak sa ešte nič nevykonalo. Pripravil sa len RDD s transformáciami, ktoré sa vykonajú vtedy, keď to bude treba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11]\n",
      "Elapsed time: 0.07054328918457031 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).take(5))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Až teraz som zavolal funkciu, ktorá vyžaduje aby sa niečo aj spočítalo. Stačilo mi ale spočítať prvých pár čísel, takže sa vykonala len časť výpočtu. Ďalšie funkcie, ktoré vracajú dáta sú napr. collect, count, ...\n",
    "\n",
    "Opatrne s týmito funkciami (hlavne s collect). Vracajú vám všetky dáta, ktoré sú výsledkom výpočtu. Ak by ich bolo veľa, tak sa ich aj tak pokúsia vrátiť a počítač, z ktorého pristupujete k sparku to nemusí zvládnuť (ak teda pristupujem k nejakému väčšiemu stroju).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999983, 999979, 999961, 999959, 999953]\n",
      "Elapsed time: 2.7241902351379395 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(nums.filter(isprime).takeOrdered(5, key = lambda x: -x))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu som už potreboval spočítať všetko na to aby som to usporiadal, tak sa to muselo vykonať všetko a chvíľu to teda trvalo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skúsme trochu reálnejší príklad\n",
    "\n",
    "zdroje:\n",
    "* https://github.com/jadianes/spark-py-notebooks\n",
    "* https://www.codementor.io/jadianes/python-spark-sql-dataframes-du107w74i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stiahnem si dáta o útokoch na počítačovú sieť\n",
    "\n",
    "Sú to dáta charakterizujúce spojenia v sieti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"data/kddcup.data.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"data/kddcup.data_10_percent.gz\")\n",
    "# f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"data/corrected.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 21M\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 1.4M Dec 11 23:39 corrected.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly 2.1M Dec 11 23:32 kddcup.data_10_percent.gz\n",
      "-rw-r--r-- 1 jakub.sevcech anomaly  18M Dec 11 23:32 kddcup.data.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vytvorím si RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všimnite si, že pracujem s komprimovanými dátami. Skutočný objem je cca. 10 násobný."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definujem schému - hlavičku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# nacitam data a nastavim im schemu\n",
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vytvorím si DataFrame veľmi podobný tomu v Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = sqlContext.createDataFrame(row_data)\n",
    "type(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Viem robiť podobné operácie ako s Pandas, akurát distribuovane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.groupBy(\"protocol_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Čo keď by som chcel k tým dátam pristupovať cez SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")\n",
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Čiže by sme vedeli Spark použiť napríklad na explikatívnu analýzu celkom veľkých objemov dát\n",
    "\n",
    "## Máme tiež knižnice, na to aby sme natrénovali aj nejaké modely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zoberme si trénovacie a testovacie dáta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size is 4898431\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "print(\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 311029\n"
     ]
    }
   ],
   "source": [
    "test_data_file = \"data/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print(\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Načítajme si ich ako zoznamy riadkov = pozorovania s atribútmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pozorovanie vyzerá nejak takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '215',\n",
       "  '45076',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opis dát je dostupný tu\n",
    "http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takéto hodnoty nadobúdajú niektoré atribúty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icmp', 'udp', 'tcp']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols = csv_data.map(lambda x: x[1]).distinct().collect()\n",
    "protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finger',\n",
       " 'netbios_dgm',\n",
       " 'name',\n",
       " 'X11',\n",
       " 'hostnames',\n",
       " 'vmnet',\n",
       " 'systat',\n",
       " 'shell',\n",
       " 'netstat',\n",
       " 'netbios_ssn',\n",
       " 'urh_i',\n",
       " 'pop_3',\n",
       " 'ldap',\n",
       " 'domain',\n",
       " 'mtp',\n",
       " 'remote_job',\n",
       " 'exec',\n",
       " 'supdup',\n",
       " 'courier',\n",
       " 'urp_i',\n",
       " 'pop_2',\n",
       " 'csnet_ns',\n",
       " 'smtp',\n",
       " 'whois',\n",
       " 'daytime',\n",
       " 'bgp',\n",
       " 'imap4',\n",
       " 'nntp',\n",
       " 'http_443',\n",
       " 'klogin',\n",
       " 'rje',\n",
       " 'IRC',\n",
       " 'link',\n",
       " 'http_8001',\n",
       " 'uucp',\n",
       " 'tftp_u',\n",
       " 'iso_tsap',\n",
       " 'uucp_path',\n",
       " 'auth',\n",
       " 'ecr_i',\n",
       " 'other',\n",
       " 'domain_u',\n",
       " 'ssh',\n",
       " 'discard',\n",
       " 'ctf',\n",
       " 'red_i',\n",
       " 'tim_i',\n",
       " 'time',\n",
       " 'login',\n",
       " 'Z39_50',\n",
       " 'ftp',\n",
       " 'telnet',\n",
       " 'ntp_u',\n",
       " 'sql_net',\n",
       " 'aol',\n",
       " 'private',\n",
       " 'gopher',\n",
       " 'efs',\n",
       " 'http_2784',\n",
       " 'ftp_data',\n",
       " 'nnsp',\n",
       " 'http',\n",
       " 'sunrpc',\n",
       " 'eco_i',\n",
       " 'harvest',\n",
       " 'kshell',\n",
       " 'echo',\n",
       " 'netbios_ns',\n",
       " 'pm_dump',\n",
       " 'printer']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
    "services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0', 'RSTR', 'SH', 'S1', 'S2', 'RSTOS0', 'REJ', 'OTH', 'SF', 'S3', 'RSTO']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = csv_data.map(lambda x: x[3]).distinct().collect()\n",
    "flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zakódujme si kategorické dáta\n",
    "\n",
    "Pre jednoduchosť to teraz môžeme spraviť ako keby to boli ordinálne premenné. V modeli explicitne povieme, že sú to kategorické premenné aby to nebral ako čísla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_labeled_point(line_split):\n",
    "    # leave_out = [41] (label)\n",
    "    clean_line_split = line_split[0:41]\n",
    "    \n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(protocols)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[2] = services.index(clean_line_split[2])\n",
    "    except:\n",
    "        clean_line_split[2] = len(services)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "        \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = csv_data.map(create_labeled_point)\n",
    "test_data = test_csv_data.map(create_labeled_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A natrénujme rozhodovací strom\n",
    "\n",
    "Používame knižnicu **mllib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 272.709 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "t0 = time.time()\n",
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n",
    "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
    "\n",
    "print(\"Classifier trained in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predikcie sa dajú spočítať takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated in 0.049 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "\n",
    "print(\"Predictions generated in {} seconds\".format(round(time.time() - t0,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A úspešnosť môžeme vyhodnotiť takto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 23.226 seconds. Test accuracy is 0.916\n"
     ]
    }
   ],
   "source": [
    "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)\n",
    "\n",
    "t0 = time.time()\n",
    "test_accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "\n",
    "print(\"Prediction made in {} seconds. Test accuracy is {}\".format(round(time.time() - t0,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pravidla zo stromu si vieme jednoducho vypísať"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 27 nodes\n",
      "  If (feature 22 <= 33.0)\n",
      "   If (feature 25 <= 0.5)\n",
      "    If (feature 36 <= 0.48)\n",
      "     If (feature 34 <= 0.91)\n",
      "      Predict: 0.0\n",
      "     Else (feature 34 > 0.91)\n",
      "      Predict: 1.0\n",
      "    Else (feature 36 > 0.48)\n",
      "     If (feature 2 in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,56.0,42.0,52.0,14.0,61.0,38.0,13.0,41.0,2.0,32.0,22.0,44.0,50.0,11.0,23.0,30.0,51.0,19.0,47.0,15.0})\n",
      "      Predict: 1.0\n",
      "   Else (feature 25 > 0.5)\n",
      "    If (feature 3 in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 2 in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,61.0,38.0,22.0,59.0,7.0,3.0,50.0,31.0,11.0,40.0,51.0,47.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 3 not in {5.0,6.0,9.0,3.0,8.0,4.0})\n",
      "     If (feature 38 <= 0.07)\n",
      "      Predict: 0.0\n",
      "     Else (feature 38 > 0.07)\n",
      "      Predict: 1.0\n",
      "  Else (feature 22 > 33.0)\n",
      "   If (feature 5 <= 0.0)\n",
      "    If (feature 2 in {19.0,35.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 not in {19.0,35.0})\n",
      "     If (feature 11 <= 0.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 11 > 0.0)\n",
      "      Predict: 0.0\n",
      "   Else (feature 5 > 0.0)\n",
      "    If (feature 29 <= 0.08)\n",
      "     If (feature 2 in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {52.0,61.0,13.0,41.0,22.0,40.0,51.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 29 > 0.08)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned classification tree model:\")\n",
    "print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Teraz by som vedel interpretovať celý strom. Stačí mi pozrieť sa čo mám v jednotlivých stĺpcoch.  Popis dát je dostupný tu: http://kdd.ics.uci.edu/databases/kddcup99/task.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
